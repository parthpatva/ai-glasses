import cv2
import pyttsx3
import speech_recognition as sr
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import threading
import time
from ultralytics import YOLO

# Initialize model and TTS engine
model = YOLO("yolov8n.pt")
engine = pyttsx3.init()
engine.setProperty("rate", 170)

LANGUAGE = 'en'

def speak(text):
    print("SPEAK:", text)
    engine.say(text)
    engine.runAndWait()

def listen_command(prompt="Say something"):
    speak(prompt)
    fs = 16000
    duration = 4
    try:
        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
        sd.wait()
        wav.write("temp.wav", fs, recording)

        recognizer = sr.Recognizer()
        with sr.AudioFile("temp.wav") as source:
            audio = recognizer.record(source)

        command = recognizer.recognize_google(audio).lower()
        print(f"RECOGNIZED: {command}")
        return command
    except Exception as e:
        print(f"Voice recognition failed: {e}")
        return input("Voice failed, type instead: ").lower()

class VideoCaptureThread:
    def __init__(self, source=0):
        self.cap = cv2.VideoCapture(source)
        self.ret, self.frame = self.cap.read()
        self.running = True
        self.thread = threading.Thread(target=self.update)
        self.thread.start()

    def update(self):
        while self.running:
            self.ret, self.frame = self.cap.read()

    def read(self):
        return self.ret, self.frame

    def stop(self):
        self.running = False
        self.thread.join()
        self.cap.release()

def detect_objects(frame):
    results = model(frame, verbose=False)
    detected = []
    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            class_id = int(box.cls[0])
            label = model.names[class_id]
            detected.append((label, x1, x2))
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    return frame, detected

def get_navigation(objects, frame_width):
    left = sum(1 for o in objects if o[1] < frame_width // 3)
    right = sum(1 for o in objects if o[2] > 2 * (frame_width // 3))
    center = sum(1 for o in objects if frame_width // 3 <= o[1] <= 2 * (frame_width // 3))

    if left == 0 and right == 0 and center == 0:
        return ("Path is clear. Move forward.",
                "रास्ता साफ है। आगे बढ़ें।")
    elif center > 0 and left == 0 and right == 0:
        return ("Obstacle ahead. Please stop.",
                "सामने बाधा है। कृपया रुकें।")
    elif left == 0:
        return ("Obstacle on right. Move left.",
                "दाईं ओर बाधा है। बाईं ओर जाएं।")
    elif right == 0:
        return ("Obstacle on left. Move right.",
                "बाईं ओर बाधा है। दाईं ओर जाएं।")
    else:
        return ("Obstacle in all directions. Please stop.",
                "हर दिशा में बाधा है। कृपया रुकें।")

def help_mode():
    cap = cv2.VideoCapture(0)
    ret, frame = cap.read()
    if not ret:
        speak("Camera not available.")
        cap.release()
        return

    frame = cv2.resize(frame, (320, 240))
    frame, detected = detect_objects(frame)
    nav_eng, nav_hin = get_navigation(detected, frame.shape[1])
    message = nav_eng if LANGUAGE == 'en' else nav_hin

    speak(message)
    cv2.putText(frame, message, (5, 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    cv2.imshow("AI Glasses - Help Mode", frame)
    cv2.waitKey(4000)
    cv2.destroyWindow("AI Glasses - Help Mode")
    cap.release()

    feedback = listen_command("Did that help? Say yes or no.")
    if "yes" in feedback:
        next_mode()
    else:
        help_mode()

def switch_mode():
    stream = VideoCaptureThread()
    last_spoken = ""
    speak("Switch mode activated. Navigation started.")
    while True:
        ret, frame = stream.read()
        if not ret:
            speak("Camera error.")
            break

        frame = cv2.resize(frame, (320, 240))
        frame, detected = detect_objects(frame)
        nav_eng, nav_hin = get_navigation(detected, frame.shape[1])
        text_to_speak = nav_eng if LANGUAGE == 'en' else nav_hin

        if text_to_speak != last_spoken:
            last_spoken = text_to_speak
            threading.Thread(target=speak, args=(text_to_speak,)).start()

        cv2.putText(frame, text_to_speak, (5, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        cv2.imshow("AI Glasses - Switch Mode", frame)

        key = cv2.waitKey(1)
        if key & 0xFF == ord('q'):
            break
        elif key & 0xFF == ord('h'):  # hotkey to switch to help mode
            break

    stream.stop()
    cv2.destroyAllWindows()
    next_mode()

def next_mode():
    speak("Would you like help or switch mode?")
    user_mode = listen_command("Say help or switch.")
    if "help" in user_mode:
        help_mode()
    elif "switch" in user_mode:
        switch_mode()
    else:
        speak("Didn't catch that. Defaulting to switch mode.")
        switch_mode()

def main():
    global LANGUAGE
    lang_input = listen_command("Please say English or Hindi to choose your language.")
    if "hindi" in lang_input:
        LANGUAGE = 'hi'
        speak("आपने हिंदी चुनी है।")
    else:
        LANGUAGE = 'en'
        speak("You selected English.")

    speak("Hey! It's your AI guiding you to your path.")
    next_mode()

if __name__ == "__main__":
    main()
