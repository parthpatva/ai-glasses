import cv2
import pyttsx3
import speech_recognition as sr
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import threading
import time
from ultralytics import YOLO

# Initialize YOLO model
model = YOLO("yolov8n.pt")

# Initialize TTS engine
engine = pyttsx3.init()
engine.setProperty("rate", 170)

# Language configuration
LANGUAGE = 'en'
MODE = 'switch'

# Voice speaking
def speak(text):
    print("SPEAK:", text)
    engine.say(text)
    engine.runAndWait()

# Listen to command via microphone without pyaudio
def listen_command(prompt="Say something"):
    speak(prompt)
    print(prompt)

    fs = 16000
    duration = 4

    try:
        recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
        sd.wait()

        wav.write("temp.wav", fs, recording)

        recognizer = sr.Recognizer()
        with sr.AudioFile("temp.wav") as source:
            audio = recognizer.record(source)

        command = recognizer.recognize_google(audio).lower()
        print(f"RECOGNIZED: {command}")
        return command
    except Exception as e:
        print(f"Voice recognition failed: {e}")
        return input("Voice failed, type instead: ").lower()

# Video capture thread
class VideoCaptureThread:
    def __init__(self, source=0):
        self.cap = cv2.VideoCapture(source)
        self.ret, self.frame = self.cap.read()
        self.running = True
        self.thread = threading.Thread(target=self.update)
        self.thread.start()

    def update(self):
        while self.running:
            self.ret, self.frame = self.cap.read()

    def read(self):
        return self.ret, self.frame

    def stop(self):
        self.running = False
        self.thread.join()
        self.cap.release()

# Object detection logic
def detect_objects(frame):
    results = model(frame, verbose=False)
    detected = []

    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            class_id = int(box.cls[0])
            label = model.names[class_id]
            detected.append((label, x1, x2))
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    return frame, detected

# Navigation logic
def get_navigation(objects, frame_width):
    left = sum(1 for o in objects if o[1] < frame_width // 3)
    right = sum(1 for o in objects if o[2] > 2 * (frame_width // 3))
    center = sum(1 for o in objects if frame_width // 3 <= o[1] <= 2 * (frame_width // 3))

    if left == 0 and right == 0 and center == 0:
        return ("Path is clear. Move forward.",
                "रास्ता साफ है। आगे बढ़ें।")
    elif center > 0 and left == 0 and right == 0:
        return ("Obstacle ahead. Please stop.",
                "सामने बाधा है। कृपया रुकें।")
    elif left == 0:
        return ("Obstacle on right. Move left.",
                "दाईं ओर बाधा है। बाईं ओर जाएं।")
    elif right == 0:
        return ("Obstacle on left. Move right.",
                "बाईं ओर बाधा है। दाईं ओर जाएं।")
    else:
        return ("Obstacle in all directions. Please stop.",
                "हर दिशा में बाधा है। कृपया रुकें।")

# Help mode: capture and give one-time guidance
def help_mode(frame):
    frame = cv2.resize(frame, (320, 240))
    frame, detected = detect_objects(frame)
    nav_eng, nav_hin = get_navigation(detected, frame.shape[1])
    speak(nav_eng if LANGUAGE == 'en' else nav_hin)
    cv2.putText(frame, nav_eng if LANGUAGE == 'en' else nav_hin, (5, 20),
                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    cv2.imshow("AI Glasses - Help Mode", frame)
    cv2.waitKey(5000)
    cv2.destroyWindow("AI Glasses - Help Mode")

# Dynamic switch mode
def switch_mode():
    stream = VideoCaptureThread()
    last_spoken = ""
    while True:
        ret, frame = stream.read()
        if not ret:
            break

        frame = cv2.resize(frame, (320, 240))
        frame, detected = detect_objects(frame)
        nav_eng, nav_hin = get_navigation(detected, frame.shape[1])
        text_to_speak = nav_eng if LANGUAGE == 'en' else nav_hin

        if text_to_speak != last_spoken:
            last_spoken = text_to_speak
            threading.Thread(target=speak, args=(text_to_speak,)).start()

        cv2.putText(frame, text_to_speak, (5, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        cv2.imshow("AI Glasses - Switch Mode", frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    stream.stop()
    cv2.destroyAllWindows()

# Main program
def main():
    global LANGUAGE, MODE

    lang_input = listen_command("Please say English or Hindi to choose your language.")
    if "hindi" in lang_input:
        LANGUAGE = 'hi'
    else:
        LANGUAGE = 'en'
    speak("You selected Hindi." if LANGUAGE == 'hi' else "You selected English.")

    mode_input = listen_command("Please say help or switch to choose your mode.")
    if "help" in mode_input:
        MODE = 'help'
    else:
        MODE = 'switch'
    speak("Help mode activated." if MODE == 'help' else "Switch mode activated.")

    # Start guidance
    speak("Hey! It's your AI guiding you to your path.")
    if MODE == 'help':
        cap = cv2.VideoCapture(0)
        ret, frame = cap.read()
        if ret:
            help_mode(frame)
        cap.release()
    else:
        switch_mode()

if __name__ == "__main__":
    main()
