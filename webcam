import cv2
import pyttsx3
import threading
import time
import speech_recognition as sr
from ultralytics import YOLO

# Load YOLOv8 model
model = YOLO("yolov8n.pt")

# TTS Engine
engine = pyttsx3.init()
engine.setProperty("rate", 170)

# Default language
language_mode = 'en'

# Text-to-Speech with language control
def speak(text):
    global language_mode
    print(f"[AI]: {text}")
    if language_mode == 'hi':
        hindi_map = {
            "Obstacle on the left. Move right.": "बाएं रुकावट है, दाईं ओर जाएं।",
            "Obstacle on the right. Move left.": "दाएं रुकावट है, बाईं ओर जाएं।",
            "Obstacle ahead. Stop.": "आगे रुकावट है, रुक जाएं।",
            "Path clear. Move forward.": "रास्ता साफ है, आगे बढ़ें।",
            "Hey! It's your AI guiding you on your path.": "नमस्ते! यह आपका एआई है जो आपके रास्ते में मदद कर रहा है।",
            "Say help for one-time guidance or switch for continuous help.": "एक बार सहायता के लिए हेल्प बोलें या निरंतर सहायता के लिए स्विच बोलें।",
            "Please say english or hindi to choose your language.": "कृपया अपनी भाषा चुनने के लिए इंग्लिश या हिंदी बोलें।"
        }
        text = hindi_map.get(text, text)
    threading.Thread(target=lambda: engine.say(text) or engine.runAndWait()).start()

# Threaded Video Capture
class VideoCaptureThread:
    def __init__(self, src=0):
        self.cap = cv2.VideoCapture(src)
        self.ret, self.frame = self.cap.read()
        self.running = True
        self.thread = threading.Thread(target=self.update, args=())
        self.thread.start()

    def update(self):
        while self.running:
            self.ret, self.frame = self.cap.read()

    def read(self):
        return self.ret, self.frame

    def stop(self):
        self.running = False
        self.thread.join()
        self.cap.release()

# Detection and guidance
def detect_and_navigate(frame):
    results = model(frame, verbose=False)
    detected_objects = []

    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            class_id = int(box.cls[0])
            label = model.names[class_id]
            center_x = (x1 + x2) // 2
            detected_objects.append(center_x)

            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)

    width = frame.shape[1]
    left = sum(1 for x in detected_objects if x < width // 3)
    right = sum(1 for x in detected_objects if x > 2 * width // 3)
    center = len(detected_objects) - left - right

    if center > 0:
        message = "Obstacle ahead. Stop."
    elif left > right:
        message = "Obstacle on the left. Move right."
    elif right > left:
        message = "Obstacle on the right. Move left."
    else:
        message = "Path clear. Move forward."

    speak(message)
    return frame, message

# Voice command listener
def listen_command(prompt=""):
    if prompt:
        speak(prompt)
    r = sr.Recognizer()
    with sr.Microphone() as source:
        print("Listening...")
        audio = r.listen(source, phrase_time_limit=3)
    try:
        command = r.recognize_google(audio).lower()
        print(f"[Voice Command]: {command}")
        return command
    except:
        return ""

# Main function
def main():
    global language_mode
    # Step 1: Ask for language
    while True:
        lang_input = listen_command("Please say english or hindi to choose your language.")
        if "hindi" in lang_input:
            language_mode = 'hi'
            speak("आपने हिंदी चुनी है।")
            break
        elif "english" in lang_input:
            language_mode = 'en'
            speak("You have chosen English.")
            break

    # Step 2: Ask for mode
    speak("Hey! It's your AI guiding you on your path.")
    while True:
        mode_input = listen_command("Say help for one-time guidance or switch for continuous help.")
        if "help" in mode_input:
            mode = "help"
            break
        elif "switch" in mode_input:
            mode = "switch"
            break

    cap = VideoCaptureThread(0)

    if mode == "help":
        time.sleep(1)
        ret, frame = cap.read()
        if ret:
            frame = cv2.resize(frame, (320, 240))
            frame, msg = detect_and_navigate(frame)
            cv2.putText(frame, msg, (10, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            cv2.imshow("AI Assistant", frame)
            cv2.waitKey(5000)
        cap.stop()

    elif mode == "switch":
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame = cv2.resize(frame, (320, 240))
            frame, msg = detect_and_navigate(frame)
            cv2.putText(frame, msg, (10, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
            cv2.imshow("AI Assistant", frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.stop()

    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
